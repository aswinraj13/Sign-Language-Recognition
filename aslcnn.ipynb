{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8aabab7-59b3-4e49-b612-daf32bbd4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db28992-65c5-4550-a791-08c723413e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21974 images belonging to 24 classes.\n",
      "Found 5481 images belonging to 24 classes.\n",
      "Class Indices: {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'O': 13, 'P': 14, 'Q': 15, 'R': 16, 'S': 17, 'T': 18, 'U': 19, 'V': 20, 'W': 21, 'X': 22, 'Y': 23}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the path to your dataset\n",
    "train_dir = r\"C:\\Users\\arekn\\OneDrive\\Desktop\\archive\\train\"\n",
    "test_dir = r\"C:\\Users\\arekn\\OneDrive\\Desktop\\archive\\test\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # 20% of data will be used for validation\n",
    ")\n",
    "\n",
    "# Use the same generator for both training and validation, but specify 'subset'\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # 80% of data for training\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,  # Still using the training directory for validation data\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # 20% of data for validation\n",
    ")\n",
    "\n",
    "# Print class indices to see the mappings of folders (letters A-Y) to numerical labels\n",
    "print(\"Class Indices:\", train_generator.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "367ed35b-2453-4c65-8010-9c003cceec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "# Model Architecture\n",
    "model = Sequential([\n",
    "    Input(shape=(64, 64, 3)),  # Define the input layer explicitly\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(train_generator.class_indices), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b66bfd9f-9dd8-49d6-9c55-34d12ac20e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arekn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 343ms/step - accuracy: 0.0701 - loss: 3.3282 - val_accuracy: 0.1394 - val_loss: 2.8082 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 216ms/step - accuracy: 0.1310 - loss: 2.8124 - val_accuracy: 0.1783 - val_loss: 2.6352 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 156ms/step - accuracy: 0.1973 - loss: 2.4872 - val_accuracy: 0.2385 - val_loss: 2.7208 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 156ms/step - accuracy: 0.2538 - loss: 2.2616 - val_accuracy: 0.3204 - val_loss: 2.0327 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 149ms/step - accuracy: 0.3142 - loss: 2.0233 - val_accuracy: 0.4527 - val_loss: 1.6742 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 149ms/step - accuracy: 0.3590 - loss: 1.8572 - val_accuracy: 0.3060 - val_loss: 2.2100 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 149ms/step - accuracy: 0.4051 - loss: 1.7251 - val_accuracy: 0.2527 - val_loss: 3.3976 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 149ms/step - accuracy: 0.4618 - loss: 1.5459 - val_accuracy: 0.5968 - val_loss: 1.1532 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 151ms/step - accuracy: 0.5083 - loss: 1.3957 - val_accuracy: 0.4461 - val_loss: 1.7921 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 151ms/step - accuracy: 0.5497 - loss: 1.2628 - val_accuracy: 0.5109 - val_loss: 1.9346 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 150ms/step - accuracy: 0.5891 - loss: 1.1623 - val_accuracy: 0.5056 - val_loss: 2.6128 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 151ms/step - accuracy: 0.6270 - loss: 1.0525 - val_accuracy: 0.5895 - val_loss: 1.6611 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 151ms/step - accuracy: 0.6482 - loss: 0.9829 - val_accuracy: 0.7630 - val_loss: 0.7018 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 151ms/step - accuracy: 0.6776 - loss: 0.9164 - val_accuracy: 0.7770 - val_loss: 0.6433 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 150ms/step - accuracy: 0.6980 - loss: 0.8557 - val_accuracy: 0.8174 - val_loss: 0.5287 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 151ms/step - accuracy: 0.7254 - loss: 0.7779 - val_accuracy: 0.8349 - val_loss: 0.4795 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 151ms/step - accuracy: 0.7331 - loss: 0.7519 - val_accuracy: 0.6844 - val_loss: 1.0454 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.7510 - loss: 0.7232 - val_accuracy: 0.7688 - val_loss: 0.7200 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 151ms/step - accuracy: 0.7742 - loss: 0.6514 - val_accuracy: 0.8269 - val_loss: 0.5364 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.7869 - loss: 0.6078 - val_accuracy: 0.7066 - val_loss: 0.9776 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.8017 - loss: 0.5687 - val_accuracy: 0.6659 - val_loss: 1.2079 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.8213 - loss: 0.5071 - val_accuracy: 0.9298 - val_loss: 0.2148 - learning_rate: 2.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.8517 - loss: 0.4316 - val_accuracy: 0.9141 - val_loss: 0.2497 - learning_rate: 2.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 153ms/step - accuracy: 0.8514 - loss: 0.4368 - val_accuracy: 0.9226 - val_loss: 0.2340 - learning_rate: 2.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.8624 - loss: 0.4066 - val_accuracy: 0.9387 - val_loss: 0.1862 - learning_rate: 2.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.8620 - loss: 0.3958 - val_accuracy: 0.8982 - val_loss: 0.2945 - learning_rate: 2.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.8700 - loss: 0.3867 - val_accuracy: 0.9367 - val_loss: 0.1964 - learning_rate: 2.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.8712 - loss: 0.3694 - val_accuracy: 0.9423 - val_loss: 0.1771 - learning_rate: 2.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 202ms/step - accuracy: 0.8784 - loss: 0.3470 - val_accuracy: 0.9471 - val_loss: 0.1681 - learning_rate: 2.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 183ms/step - accuracy: 0.8769 - loss: 0.3498 - val_accuracy: 0.9535 - val_loss: 0.1431 - learning_rate: 2.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 154ms/step - accuracy: 0.8830 - loss: 0.3372 - val_accuracy: 0.9606 - val_loss: 0.1220 - learning_rate: 2.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.8893 - loss: 0.3242 - val_accuracy: 0.9599 - val_loss: 0.1314 - learning_rate: 2.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.8890 - loss: 0.3242 - val_accuracy: 0.9515 - val_loss: 0.1535 - learning_rate: 2.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 153ms/step - accuracy: 0.8918 - loss: 0.3151 - val_accuracy: 0.9593 - val_loss: 0.1353 - learning_rate: 2.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 153ms/step - accuracy: 0.8977 - loss: 0.3078 - val_accuracy: 0.9637 - val_loss: 0.1104 - learning_rate: 2.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.8967 - loss: 0.3026 - val_accuracy: 0.9513 - val_loss: 0.1520 - learning_rate: 2.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 303ms/step - accuracy: 0.8992 - loss: 0.3017 - val_accuracy: 0.9350 - val_loss: 0.1901 - learning_rate: 2.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 183ms/step - accuracy: 0.9012 - loss: 0.2963 - val_accuracy: 0.9593 - val_loss: 0.1236 - learning_rate: 2.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.9041 - loss: 0.2748 - val_accuracy: 0.9602 - val_loss: 0.1121 - learning_rate: 2.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 153ms/step - accuracy: 0.9054 - loss: 0.2890 - val_accuracy: 0.9673 - val_loss: 0.0953 - learning_rate: 2.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 153ms/step - accuracy: 0.9098 - loss: 0.2684 - val_accuracy: 0.9579 - val_loss: 0.1273 - learning_rate: 2.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 154ms/step - accuracy: 0.9131 - loss: 0.2564 - val_accuracy: 0.9686 - val_loss: 0.0969 - learning_rate: 2.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 153ms/step - accuracy: 0.9113 - loss: 0.2587 - val_accuracy: 0.9281 - val_loss: 0.2269 - learning_rate: 2.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 153ms/step - accuracy: 0.9110 - loss: 0.2659 - val_accuracy: 0.9719 - val_loss: 0.0881 - learning_rate: 2.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 153ms/step - accuracy: 0.9178 - loss: 0.2454 - val_accuracy: 0.9489 - val_loss: 0.1644 - learning_rate: 2.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 153ms/step - accuracy: 0.9182 - loss: 0.2436 - val_accuracy: 0.9743 - val_loss: 0.0863 - learning_rate: 2.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.9127 - loss: 0.2457 - val_accuracy: 0.9756 - val_loss: 0.0830 - learning_rate: 2.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.9225 - loss: 0.2252 - val_accuracy: 0.9732 - val_loss: 0.0802 - learning_rate: 2.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.9224 - loss: 0.2336 - val_accuracy: 0.9688 - val_loss: 0.0984 - learning_rate: 2.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 152ms/step - accuracy: 0.9276 - loss: 0.2225 - val_accuracy: 0.9735 - val_loss: 0.0906 - learning_rate: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('sign_language_model.keras', save_best_only=True, monitor='val_loss')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping, checkpoint, reduce_lr]\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "model.save('sign_language_final_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91276dad-b3f1-4e50-8eb7-93173570a767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\arekn\\AppData\\Local\\Temp\\tmpa7o520jc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\arekn\\AppData\\Local\\Temp\\tmpa7o520jc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\arekn\\AppData\\Local\\Temp\\tmpa7o520jc'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name='keras_tensor_28')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 24), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2582020088912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020087568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020090640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020086416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020089296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020090448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020091408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020092560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020092944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020091024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020091792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020092752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020093712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020094864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020095248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020094480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020094096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020095056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582020093328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582026880208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582026880400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2582026880976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Model successfully converted to TFLite and saved as sign_language_model.tflite\n"
     ]
    }
   ],
   "source": [
    "# Convert the Keras model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "tflite_model_path = 'sign_language_model.tflite'\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"Model successfully converted to TFLite and saved as {tflite_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54adf6ff-4b9e-47f1-8c4c-b7d38af1d205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n",
      "Predicted class: N\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the image you want to test\n",
    "img_path = r\"C:\\Users\\arekn\\OneDrive\\Desktop\\archive\\Test\\N\\480_N.jpg\"\n",
    "img = image.load_img(img_path, target_size=(64, 64))\n",
    "\n",
    "# Preprocess the image to match the input format of the model\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "img_array = img_array / 255.0  # Normalize the image just like in training\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('sign_language_final_model.keras')\n",
    "\n",
    "# Make a prediction\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# Get the predicted class index\n",
    "predicted_class_index = np.argmax(predictions[0])\n",
    "\n",
    "# Print the predicted class\n",
    "class_indices = train_generator.class_indices  # Use the same class indices as used during training\n",
    "class_labels = {v: k for k, v in class_indices.items()}  # Invert the dictionary to map indices back to class labels\n",
    "predicted_label = class_labels[predicted_class_index]\n",
    "\n",
    "print(f\"Predicted class: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5374a62-45c0-4a1c-a3a8-dc6b0e94c066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7172 images belonging to 24 classes.\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 298ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arekn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 158ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      1.00      1.00       331\n",
      "           B       1.00      1.00      1.00       432\n",
      "           C       1.00      1.00      1.00       310\n",
      "           D       0.97      1.00      0.99       245\n",
      "           E       1.00      0.96      0.98       498\n",
      "           F       1.00      1.00      1.00       247\n",
      "           G       1.00      0.98      0.99       348\n",
      "           H       0.99      1.00      0.99       436\n",
      "           I       1.00      1.00      1.00       288\n",
      "           K       1.00      0.98      0.99       331\n",
      "           L       1.00      1.00      1.00       209\n",
      "           M       0.95      0.99      0.97       394\n",
      "           N       0.99      0.95      0.97       291\n",
      "           O       1.00      1.00      1.00       246\n",
      "           P       1.00      1.00      1.00       347\n",
      "           Q       1.00      1.00      1.00       164\n",
      "           R       0.97      0.98      0.97       144\n",
      "           S       0.93      0.99      0.96       246\n",
      "           T       1.00      0.96      0.98       248\n",
      "           U       1.00      0.99      0.99       266\n",
      "           V       0.96      0.98      0.97       346\n",
      "           W       0.98      1.00      0.99       206\n",
      "           X       0.96      1.00      0.98       267\n",
      "           Y       1.00      0.96      0.98       332\n",
      "\n",
      "    accuracy                           0.99      7172\n",
      "   macro avg       0.99      0.99      0.99      7172\n",
      "weighted avg       0.99      0.99      0.99      7172\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load your test data (assuming you have test_generator or test data in a similar format)\n",
    "# For example, using ImageDataGenerator for test data\n",
    "test_datagen = image.ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'C:\\\\Users\\\\arekn\\\\OneDrive\\\\Desktop\\\\archive\\\\test',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Important to set shuffle=False for consistent results\n",
    ")\n",
    "\n",
    "# Load the saved .keras model\n",
    "model = tf.keras.models.load_model('sign_language_final_model.keras')\n",
    "\n",
    "# Predict the classes for the test data\n",
    "y_pred = model.predict(test_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Get the ground truth labels from the test generator\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Get class labels mapping from the generator (mapping from indices to class names)\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_true, y_pred_classes, target_names=class_labels)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490b31e-910a-4983-8d05-9f6ca7909f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
