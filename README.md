# Sign-Language-Recognition


As part of the Mobile Application Development course at Amrita School of Computing, we developed SignLanguageAI: Communication , an AI-powered app focused on emotion detection through facial expression analysis. The app is designed to assist within the Accessibility & Assistive Technology domain, specifically in Assistive Technology  and Gesture Recognition .
Our project utilizes the Sign Language dataset to train several deep learning models, including CNN, Vision Transformer, and MobileNetV2. Testing revealed that CNN provided the highest accuracy at 97%, followed by Vision Transformer at 78% and MobileNetV2 at 67.4%. The dataset can be accessed here: .	
For a demonstration of the app in action, https://www.linkedin.com/posts/aswinraj-ek_as-part-of-the-mobile-application-development-ugcPost-7256930412663328769-RZM0 .
The complete source code for SignLanguageAI is open-source and available on GitHub: https://github.com/aswinraj13/Sign-Language-Recognition.git .
